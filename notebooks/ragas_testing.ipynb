{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from datasets import load_dataset\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.document_loaders import HuggingFaceDatasetLoader\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the existing Chroma instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma(persist_directory=\"chroma\", embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='and I\" (DW) This year, the jury chose two winners in the category Online: \"Digital journalism has a lot to offer. The two prizewinners represent a different approach in an interesting way and show how journalism generally evolves with multimedia possibilities,\" explains the jury. In the first contribution, Christian Salewski und Felix Rohrbeck track the disposal of electronic scrap in Germany and find out that it isn\\'t always legal and fair. In the second contribution, a group of Deutsche Welle trainees asked their grandmothers from Belarus, Brazil, Chile, China, Kenya and Germany about their'),\n",
       " Document(page_content='is, in journalism, if we gather the \"facts,\" we can usually find the answers to what we\\'re looking for.  When it comes to God, Jesus and the Holy Spirit, those answers rest in faith. As a journalist, I seek intellectual certainty. When it came to my faith, I felt intellectually embarrassed. There was so much I just couldn\\'t explain. When I started working on a documentary about the growth of atheism, I found myself in a profound place of reflection.  In the days when I thought I was going to pursue a life of ministry, I experienced and felt many things that were unexplainable.  What was that?'),\n",
       " Document(page_content='data, its approach to social media and what it takes to produce a successful online video. Award winners were then hosted at a special lunch by London Bureau Chief Tommy Evans and Chief International Correspondent Christiane Amanpour. The awards were an inspirational testament to the power of good journalism and everyone involved with the event was thrilled to host the winners for this special program of events. CNN Journalist of the Year 2015 and Winner in the category Radio: Stephanie Doetzer: \"Take care, Habibi\" (DRadio Wissen) Stories about the civil war in Syria are often quite abstract.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"What is the article about?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize language model\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "  llm = llm,\n",
    "  chain_type = \"stuff\",\n",
    "  retriever = retriever,\n",
    "  return_source_documents = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who bit Jon Huntsman in 2011\"\n",
    "# result = rag_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation the RAG system using RAGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contexts = [doc.page_content for doc in result[\"source_documents\"]]\n",
    "# formatted_context = pretty_print_docs(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ragas.ragas_pipeline import get_context_and_answer\n",
    "from src.ragas.ragas_utils import load_evaluation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = load_evaluation_data('data/evaluation_set.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pick one sample for testing from eval_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from typing import Dict, List\n",
    "def get_context_and_answer(\n",
    "    evaluation_data: List[Dict[str, List[str]]],\n",
    "    rag_chain, \n",
    ") -> List[Dict[str, str]]:\n",
    "    \"\"\"Retrieves context and generates answers for each question in the evaluation data.\n",
    "\n",
    "    Args:\n",
    "        evaluation_data (Dict[str, List[str]]): A dictionary containing:\n",
    "            - \"questions\": A list of questions.\n",
    "            - \"ground_truths\": A list of corresponding ground truth answers.\n",
    "        rag_chain: The RAG chain instance to use for retrieval and generation.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, str]]: A list of dictionaries, each containing:\n",
    "            - \"question\": The original question.\n",
    "            - \"context\": A string of concatenated relevant contexts.\n",
    "            - \"answer\": The generated answer from the RAG chain.\n",
    "            - \"ground_truth\": The ground truth answer (from the evaluation data).\n",
    "    \"\"\"\n",
    "\n",
    "    results = {\n",
    "        \"question\": [],\n",
    "        \"contexts\": [],\n",
    "        \"answer\": [],\n",
    "        \"ground_truth\": [],\n",
    "    }\n",
    "\n",
    "    for question, ground_truth in zip(\n",
    "        evaluation_data[\"questions\"], evaluation_data[\"ground_truths\"]\n",
    "    ):\n",
    "        response = rag_chain.invoke(question)\n",
    "        contexts_list = [doc.page_content for doc in response[\"source_documents\"]]\n",
    "                \n",
    "        results[\"question\"].append(question)\n",
    "        results[\"contexts\"].append(contexts_list)\n",
    "        results[\"answer\"].append(response[\"result\"])\n",
    "        results[\"ground_truth\"].append(ground_truth)\n",
    "        \n",
    "    dataset = Dataset.from_dict(results)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick one sample from the evaluation data\n",
    "question = eval_data['questions'][0]\n",
    "ground_truth = eval_data['ground_truths'][0]\n",
    "\n",
    "test_eval = {\"questions\": [question], \"ground_truths\": [ground_truth]}\n",
    "\n",
    "test_data = get_context_and_answer(evaluation_data=test_eval, rag_chain=rag_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abdbe2bd6390492c88fb4af8938ff468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    answer_correctness,\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    ")\n",
    "\n",
    "# testset = get_context_and_answer(eval_data, rag_chain)\n",
    "\n",
    "# evaluating test set on listed metrics\n",
    "result = evaluate(\n",
    "    dataset=test_data,\n",
    "    metrics=[\n",
    "        answer_correctness,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_precision\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>answer</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>answer_correctness</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What upcoming animated project will feature Ad...</td>\n",
       "      <td>[(The Hollywood Reporter)The skies over Gotham...</td>\n",
       "      <td>The upcoming animated project that will featur...</td>\n",
       "      <td>Adam West and Burt Ward will be reprising thei...</td>\n",
       "      <td>0.744099</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What upcoming animated project will feature Ad...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [(The Hollywood Reporter)The skies over Gotham...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  The upcoming animated project that will featur...   \n",
       "\n",
       "                                        ground_truth  answer_correctness  \\\n",
       "0  Adam West and Burt Ward will be reprising thei...            0.744099   \n",
       "\n",
       "   faithfulness  answer_relevancy  context_precision  \n",
       "0           1.0               0.0                1.0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.to_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-optimization-cnn-dailymail-hiPg4Kip-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
